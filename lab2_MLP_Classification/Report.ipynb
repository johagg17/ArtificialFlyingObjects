{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56a7381",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c2fbf",
   "metadata": {},
   "source": [
    "**Name:** Johannes HÃ¤gglund \\\n",
    "**Date:** 2021 - 12 - 05 \n",
    "\n",
    "## Introduction\n",
    "This notebook includes the answers to the questions, and explanations for different solutions. <br> \n",
    "This lab was divided into four different exercises, and for each different questions will be answered. \n",
    "## Result\n",
    "\n",
    "### Exercise 0201\n",
    "**Question 1:** Can the problem be solved with a single hidden node? What happend? <br>\n",
    "**Answer 1:** No, according to the image below, a single node can't seperate the two classes. Though, by increasing the data to 1000 points, a single node achieve 100% training accruacy.  <br><br>\n",
    "<center><strong>Training with 100 data points</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0201/boundary_fail.png\" alt=\"pic3\" width=\"400\"/>\n",
    "<center><strong>Training with 1000 data points</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0201/Boundary.png\" alt=\"pic3\" width=\"400\"/>\n",
    "\n",
    "### Exercise 0202\n",
    "#### Task1\n",
    "**Question 1:** What is the obtained validation performance by traning with 100 points, and validating on 1000 points, using a single node?<br>\n",
    "**Answer 1:** The obtained validation performance is 50% accuracy. The result can be seen in the image below:\n",
    "<center><strong>Stats</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0202/Task1/result.png\" alt=\"pic3\" width=\"400\"/>\n",
    "\n",
    "#### Task2\n",
    "This task aimed to overtrain the model, and obtain a training accuracy over 95%. The final result can be seen in the image below, where the model obtain 99% accuracy. <br>\n",
    "\n",
    "<center><strong>Stats</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0202/Task2/result.png\" alt=\"pic3\" width=\"400\"/>\n",
    "\n",
    "#### Task3\n",
    "**Question 1:** When you overtrained in the previous question, how much did the validation loss increase, compared to the linear model of Task 1? <br>\n",
    "**Answer 1:** About 1 loss unit. This can be seen in the graph below, though the comparison is a bit vague due to running two models with two different epochs. \n",
    "<center><strong>Comparison</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0202/Task3/loss_Validation.svg\" alt=\"pic3\" width=\"400\"/>\n",
    "<br><br>\n",
    "\n",
    "**Question 2:** What is the optimal number of hidden nodes for the syn2 dataset in order to maximize your validation performance? <br>\n",
    "**Answer 2:** I ran an comparison between 7 different number of hidden nodes: [2, 4, 5, 7, 10, 50, 100], all with 50 epochs, and I found that 10 hidden nodes were optimal to get a maximized validation performance. The comparison can be seen in the image below: \n",
    "<center><strong>Comparison</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0202/Task3/loss_Validation_optimalHiddennodes.svg\" alt=\"pic3\" width=\"400\"/>\n",
    "The light-blue curve corresponds to the model with 10 hidden nodes, which got the lowest valdiation loss compared the other models. \n",
    "<br> <br>\n",
    "\n",
    "**Question 3:** How many hidden nodes do you need to find a reasonable solution to the problem? Extra: Can you figure out why this many? <br>\n",
    "**Answer 3:** I conclude that 7 hidden nodes are required to find a reasonable solution. The data distribution was analyzed, and as it is formed as an circle, I tried to draw the boundary by lines, in other words, lines that forms a circle. From my drawing, I concluded 7 lines are optimal, thus 7 hidden nodes. The obtained solution can be shown in the figure below: <br> \n",
    "<center><strong>Result using 7 hidden nodes</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0202/Task3/Result.png\" alt=\"pic3\" width=\"400\"/>\n",
    "\n",
    "### Exercise0203\n",
    "#### Task 1\n",
    "**Question 1:** Even though the validation error is most likely still larger than the training error why do we not see any overtraining of the model? (Hint: What is it that typically causes overfitting?) <br>\n",
    "\n",
    "**Answer 1:** The cause of overfitting is when we train the model on both data distribution and noise, in this case, there is no noise introduced, thus no overfitting. \n",
    "#### Task 2\n",
    "**Question 1:** How many nodes do you have for opitimal validation performance, i.e. more hidden nodes results in overtraining? <br>\n",
    "**Answer 1:** From my obtained results I conclude that 60 hidden nodes is the model which obtains the best validation performance, this result can be shown in the image below, where the best curve corresponds to the light-blue curve: \n",
    "<center><strong>Comparison</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0203/Task2/loss_Validation.svg\" alt=\"pic1\" width=\"400\"/>\n",
    "<br>\n",
    "\n",
    "#### Task 3\n",
    "For this task the regularization term was introduced to the model. I ran comparison between 10 different l2 values, [0, 0.001, 0.002, 0.003, 0.005, 0.008, 0.009, 0.01, 0.05, 0.08] and then select the model with best validation performance. Also, each model were trained with a learning rate 0.015, 100 epochs and default batch_size. The obtained validation loss can be seen below:\n",
    "<center><strong>Comparison</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0203/Task3/loss_Validation.svg\" alt=\"pic1\" width=\"400\"/>\n",
    "The blue curve in the bottom corresponds to l2 term = 0.001, and the orange above corresponds to the model without regularization. From this I conclude, for this problem it makes sense to introduce regularization, but in order to obtain lower validation loss, tuning batch size and learning rate might improve the validation performance. \n",
    "\n",
    "### Exercise0204\n",
    "#### Task1\n",
    "For this task, the best MLP architecture had found by tuning different parameters. At first, I tuned over hidden nodes and layers, but as layers growed, the performance decreased and got worse, thus I tuned over hidden nodes and learning rate. For hidden nodes: [2, 5, 10, 20, 50, 100], and learning rate: [0.001, 0.005, 0.006, 0.007, 0.01]. All models ran over 100 epochs and a l2 term = 0.002. The best obtained model can be seen in the image below, where the model with best validation performance is the green curve. \n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0204/Task1/loss_Validation_all.svg\" alt=\"pic1\" width=\"400\"/>\n",
    "It may be hard to interpret the image, therefore a more specific image was obtained. \n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0204/Task1/loss_Validation1.svg\" alt=\"pic1\" width=\"400\"/>\n",
    "The obtained results for the green curve are the following:\n",
    "<ul> \n",
    "    <li>Validation loss = 0.3403 </li>\n",
    "    <li>Training loss = 0.6471 </li>\n",
    "    <li>Test accuracy = 88.86% </li>\n",
    "    <li>Hidden nodes =  100</li>\n",
    "    <li>Learning rate = 0.001</li>\n",
    "</ul>  \n",
    "<center><strong>Confusion matrix test data</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0204/Task1/confusion_matrix.png\" alt=\"pic1\" width=\"400\"/>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Task2\n",
    "My obtained results can be shown in the pictures below: <br>\n",
    "\n",
    "After testing different parameters such as batch_size, the architecture for the MLP, and learning rate. \n",
    "\n",
    "At first, I tried to grow the epochs to almost 2000, which for me caused the training loss to decrease to a very low value, but still not 100%. I concluded that the more epochs, the better it will learn, probably because of the data distribution. With lower epochs, the model will only learn the points in the middle. With higher epochs, it will be able to learn the points that are ranged longer from the origin. <br>\n",
    "From this conclusion, I decreased the epochs and started to tune the architecture (hidden nodes, hidden layers), and the obtained parameters to achieve 100% accuracy can be seen below: \n",
    "<br>\n",
    "<ul> \n",
    "    <li>Batch Size = 50 </li>\n",
    "    <li>Hidden nodes = 16 </li>\n",
    "    <li>Hidden layers = 4 </li>\n",
    "    <li>Learning rate = 0.005 </li>\n",
    "    <li>Weight decay (L2 term) = 0 </li>\n",
    "    <li>Epochs = 500 </li>\n",
    "</ul>  \n",
    "\n",
    "Why I have chosen Weight decay = 0, is because to obtain as much as overfitting as possible, weigth decay should be 0, thus no regularization is included into the model. <br><br>\n",
    "The images for this task can be seen below:<br><br>\n",
    "\n",
    "\n",
    "<center><strong>Decision Boundary</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0204/Task2/decision_boundary.PNG\" alt=\"pic1\" width=\"400\"/>\n",
    "<br>\n",
    "\n",
    "<center><strong>Training loss with smoothing</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0204/Task2/Loss_Train_withSmooting.svg\" alt=\"pic2\" width=\"390\"/>\n",
    "<br>\n",
    "\n",
    "<center><strong>Training loss without smoothing</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0204/Task2/Loss_Train_withoutSmoothing.svg\" alt=\"pic2\" width=\"390\"/>\n",
    "<br>\n",
    "\n",
    "<center><strong>Stats of the obtained model</strong></center>\n",
    "<img src=\"../lab2_MLP_Classification/Images/Exercise0204/Task2/stats.PNG\" alt=\"pic3\" width=\"390\"/>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
