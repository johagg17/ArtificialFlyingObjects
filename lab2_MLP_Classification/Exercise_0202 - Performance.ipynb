{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"font-size:40px;\">Exercise II: Performance</h1></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 2 - Performance\n",
    "\n",
    "From now on we will talk about *performance*! It can be performance of a trained model on the training dataset or the performance on the validation dataset. What do we mean by performance?  For classification problems we will provide 4 different measurements as returned by a call to the *stats_class* function. They are:\n",
    "* Sensitivity = fraction of correctly classified \"1\" cases\n",
    "* Specificity = fraction of correctly classified \"0\" cases\n",
    "* Accuracy = fraction of correctly classified cases\n",
    "* loss = cross-entropy error (so low loss means good performance!)\n",
    "\n",
    "A suggestion for you is to either use accuracy or loss as your performance measure.\n",
    "\n",
    "**Note:** Use a fixed random seed for this exercise since you will compare between runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: syn1 - syn3\n",
    "Three different synthetic classification problems will be used. They are all 2D binary classification problems which allows for an easy visual inspection of the different classes and the decision boundary implemented by the network. They are called *syn1, syn2* and *syn3*. Each of these datasets are generated \"on the fly\" each time. They come from various normal distributions. Since they are generated using random numbers it means that each time you generate the data it will be slightly different from next time. You can control this by having a fixed *seed* to the random number generator. The cell \"PlotData\" will plot these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows us to edit imported files without restarting the kernel for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Hacky solution to access the global utils package\n",
    "import sys,os\n",
    "sys.path.append(os.path.dirname(os.path.realpath('')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0.1-cp39-cp39-win_amd64.whl (7.2 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\johan\\anaconda3\\envs\\deeplearn\\lib\\site-packages (from scikit-learn->sklearn) (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\johan\\anaconda3\\envs\\deeplearn\\lib\\site-packages (from scikit-learn->sklearn) (1.21.4)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.1 sklearn-0.0 threadpoolctl-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from config import LabConfig\n",
    "from dataset import MLPData\n",
    "from utils.model import Model\n",
    "from utils.progressbar import LitProgressBar\n",
    "from utils.model import Model\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import (\n",
    "    plot,\n",
    "    progressbar\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = LabConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the MLP model\n",
    "\n",
    "This cell defines the MLP model. There are a number of parameters that is needed to \n",
    "define a model. Here is a list of them: **Note:** They can all be specified when you call\n",
    "this function in later cells. The ones specified in this cell are the default values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                inp_dim=None,         \n",
    "                hidden_nodes=1,                      # number of nodes in hidden layer\n",
    "                num_out=None,\n",
    "                **kwargs\n",
    "            ):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(inp_dim, hidden_nodes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_nodes, num_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function that allow us to convert numpy to pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy2Dataloader(x,y, batch_size=25, num_workers=10,**kwargs):\n",
    "    return DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.from_numpy(x).float(), \n",
    "            torch.from_numpy(y).unsqueeze(1).float()\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we convert our dataset into a pytorch Dataset. Thereafter we load it into our DataLoader. Note that we here define the batch_size and the number of workers that should be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "x_train, d_train = MLPData.syn2(100)\n",
    "x_val, d_val = MLPData.syn2(1000)\n",
    "\n",
    "train_loader = numpy2Dataloader(x_train,d_train)\n",
    "val_loader =  numpy2Dataloader(x_val,d_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Setup our local config that should be used for the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'max_epochs':10,\n",
    "    'model_params':{\n",
    "        'inp_dim':x_train.shape[1],         \n",
    "        'hidden_nodes':1,   # activation functions for the hidden layer\n",
    "        'num_out':1 # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "    },\n",
    "    'criterion':torch.nn.BCELoss(), # error function\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.005,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, put everything together and call on the trainers fit method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(MLP(**config[\"model_params\"]),**config)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "            max_epochs=config['max_epochs'], \n",
    "            gpus=cfg.GPU,\n",
    "            logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "            callbacks=[LitProgressBar()],\n",
    "            progress_bar_refresh_rate=1,\n",
    "            weights_summary=None, # Can be None, top or full\n",
    "            num_sanity_val_steps=10,   \n",
    "        )\n",
    "trainer.fit(\n",
    "    model, \n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloaders=val_loader\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the stats function to print out statistics for classification problems\n",
    "plot.stats_class(x_train, d_train, 'Training', model)\n",
    "plot.stats_class(x_val, d_val, 'Validation', model)\n",
    "\n",
    "# Show the decision boundary for the training dataset\n",
    "plot.decision_bondary(trainer.lightning_module, x_train, d_train)\n",
    "\n",
    "# If you uncomment this one you will see how the decsion boundary is with respect to the validation data\n",
    "plot.decision_bondary(trainer.lightning_module,x_val, d_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Train a classifier for the *syn2* dataset. Use the a validation dataset (val_loader) as an estimate of the *true* performance. Since we generate these datasets we can allow for a relatively large validation dataset in order to get a more accurate estimation of *true* performance. The default value in the cell is to generate 1000 validation datapoints. \n",
    "\n",
    "Now, use syn2 dataset with 100 training data points and train a *linear* MLP to separate the two classes, i.e. use a single hidden node.\n",
    "\n",
    "**TODO:** What is the performance you get on the validation dataset?\n",
    "\n",
    "## Task 2\n",
    "This task should overtrain the MLP! Increase the number of hidden nodes to get better and better training performance.\n",
    "\n",
    "**Hint:** Overtraining here often means finding good local minimum of the error function, which may require some tuning of the learning parameters. This means that you may have to change the learning rate, increase the number of epochs and use \"better\" minimization methods. Even though we have not yet talked about the *Adam* minimization method, it is generally better than vanilla *stochastic gradient descent*. It is therefore used in the cells below as the default minimizer. Also you may want to change the size of the \"batch_size\" parameter. It is by default using all data.\n",
    "\n",
    "**TODO:** How many hidden nodes do you need to reach an accuracy >95% on your training dataset?\n",
    "\n",
    "## Task 3\n",
    "Now you are going to use the *syn3* dataset. So, use **150** training datapoints from the synthetic dataset 3 and train an MLP to separate the two classes. Also use about 1000 datapoints for validation.\n",
    "\n",
    "**TODO:** When you overtrained in the previous question, how much much did the validation *loss* increase, compared to the linear model of Task 2?\n",
    "\n",
    "**TODO:** What is the optimal number of hidden nodes for the syn2 dataset in order to maximize your validation performance?\n",
    "\n",
    "**TODO:** How many hidden nodes do you need to find a reasonable solution to the problem?  Extra: Can you figure out why this many?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ccf88e37874d44b4dfe33c31e1bb4a10ca4e414e0a68744582aebd290f71bcd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
