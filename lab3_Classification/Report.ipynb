{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Johannes HÃ¤gglund\\\n",
    "**Date:** 2021-12-12\n",
    "\n",
    "## Introduction\n",
    "For this lab I was given the task to develop and evaluate a CNN to get a better understanding of the architecture. \n",
    "This included evaluation, tuning and conclusions of the final obtained model. \n",
    "\n",
    "## Result\n",
    "### Metrics\n",
    "#### Todo1\n",
    "**Question:** Does a high accuracy impy a good model? <br>\n",
    "**Answer:** No, not usually. When developing models it's important to be careful about high accuracy -> good performance, this implicity does not hold in all situations. For example, a model that achieve a very high training accuracy will not tell if the model is generally good in performance, since high training accuracy could indicate on overfitted model. An overfitted model is a model that does not generalize very well on unseen data. <br> \n",
    "Another case when we could achieve high accuracy is when the model is capable of predicting the majority of classes. <br><br>\n",
    "As conclusion, we should not rely on only accuracy as metric, there exists many other metrics that are more capable of describing the performance, e.g. precision, recall, F1 score, etc.  \n",
    "\n",
    "#### Todo2\n",
    "**Task:** Find an alternative metric which can show similar or better precision than accuracy.  <br><br>\n",
    "For this task I used precision metric, which is calculated as following: <br><br>\n",
    "<center>  $Precision = \\frac{TP}{TP + FP}$ </center> <br>\n",
    "The precision for both training and test-set can bee seen in the table below: \n",
    "<ul>\n",
    "    <li>$ Precision_{train} = 1.0 $</li>\n",
    "    <li>$ Precision_{test} = 0.4476 $</li>\n",
    "</ul>\n",
    "From the obtain precision during training, it makes sense that the value is 1 since the model is correctly classifying all data points. So when dealing with training data, the model is precise and can predict correctly with 100% of the time. For test data, the model can predict correctly 44% of the time. \n",
    "\n",
    "**Task: Comparison between metrics:** <br>\n",
    "Additional, I chose to compare the different metrics, which the data is fetched from tensorboard. The metrics I have chosen are **Accuracy, Precision, AUROC and F1 score** and the result can be seen in the table below: <br>\n",
    "\n",
    "| | Accuracy  | Precision | AUROC | F1-score | \n",
    " | --- | ---  | --- | --- | --- |\n",
    " | Train data | 0.95 | 0.95  | 0.74 | 0.95\n",
    " | Val data   | 0.43 | 0.413    | 0.44 | 0.43\n",
    "\n",
    "\n",
    "\n",
    "### Architecture\n",
    "\n",
    "#### Overfit\n",
    "The aim for this task was to overfit the model as much as possible. I did this by removing the dropout (since it works as regularization), removing the second convolutional layer and by increasing the number epochs from 5 to 40. The reuslt can be shown in the images below, where **accuracy**, **loss** and **F1-score** are shown for both training and validation. <br><br>\n",
    "\n",
    "<center><strong>Accuracy for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/ArchitectureTask/Overfit/Accuracy_Train.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/ArchitectureTask/Overfit/Accuracy_Validation.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><strong>Loss for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/ArchitectureTask/Overfit/Loss_Train.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/ArchitectureTask/Overfit/loss_Validation.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><strong>F1-score for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/ArchitectureTask/Overfit/F1_Train.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/ArchitectureTask/Overfit/F1_Validation.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>  \n",
    "    \n",
    "<br>\n",
    "\n",
    "<p>From the obtained results, as seen in the images above, I can conclude that the minimal changes I made to the architecture, caused the model to overfit, and almost obtained 100% accuracy in training with an increasing validation loss. Further changes could be to expand the feedforward network by more layers, and see if there are any changes.</p>\n",
    "\n",
    "#### Modification\n",
    "For this task, I investigated the performance by increasing the complexity much and just a little modification. The intention behind this was to look for the underfitting pattern. Since the dataset is very small, and simple, it should not be required to have a complex model. A way too complex model could in this scenario cause underfitting. <br>\n",
    "For the model I ran the following architectures:\n",
    "<ul>\n",
    "    <li>Kernel size = 11, padding = 5, conv layers = 5</li>\n",
    "    <li>Kernel size = 3, padding = 1, conv layers = 3</li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "For the first modification, the results in term of accuracy, f1-score and loss can be shown below. From these images, we can clearly see that as the complexity increases in the model, underfitting is caused. <br>\n",
    "<center><strong>Accuracy for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/ArchitectureTask/ArchModification/Accuracy_Train.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/ArchitectureTask/ArchModification/Accuracy_Validation.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><strong>Loss for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/ArchitectureTask/ArchModification/Loss_Train.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/ArchitectureTask/ArchModification/loss_Validation.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><strong>F1-score for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/ArchitectureTask/ArchModification/F1_Train.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/ArchitectureTask/ArchModification/F1_Validation.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>  \n",
    "\n",
    "\n",
    "Second architecture can be shown in the images below: <br>\n",
    "\n",
    "<center><strong>Accuracy for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/ArchitectureTask/ArchModification/Accuracy_Train2.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/ArchitectureTask/ArchModification/Accuracy_Validation2.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><strong>Loss for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/ArchitectureTask/ArchModification/Loss_Train2.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/ArchitectureTask/ArchModification/loss_Validation2.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><strong>F1-score for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/ArchitectureTask/ArchModification/F1_Train2.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/ArchitectureTask/ArchModification/F1_Validation2.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table> \n",
    "\n",
    "\n",
    "\n",
    "### Hyperparameter tuning\n",
    "This task was to hyperparameter tune the model. This included tuning of learning rate, dropout, channels in the convlayers, number of convlayers, hidden nodes in the feedforward network and regularization term. <br>\n",
    "My final model architecture is the following: <br>\n",
    "\n",
    "<ul>\n",
    "    <li>epochs = 40</li>\n",
    "    <li>number of conv layers = 3</li>\n",
    "    <li>output channels: (32, 128, 150)</li>\n",
    "    <li>Hidden nodes in fc1 = 150</li> \n",
    "    <li>(kernel size, padding) = (3, 1)</li>\n",
    "    <li>max pooling = 2x2 </li> \n",
    "    <li>dropout = 0.8 </li>\n",
    "    <li>learning rate = 0.002</li>\n",
    "    <li>l2 regularization = 0.0003</li>\n",
    "</ul>\n",
    "The results of the accuracy and F1-score can be shown below: <br>\n",
    "<center><strong>Accuracy for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/Hypertune/Accuracy_Train.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/Hypertune/Accuracy_Validation.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><strong>F1-score for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/Hypertune/F1_Train.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/Hypertune/F1_Validation.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "<br><br>\n",
    "<center><strong>Confusion matrix on test-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><img src=\"Images/Hypertune/confusionm.PNG\" alt=\"Drawing\" style=\"width: 100%;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "From the result I can conclude that, compared to the first task, my tuning improved the model to almost 88% accuracy in validation, and training accuracy did reach approximately 98%. Though, in terms of validation, I think that the performance is still a bit too vague. Due to the data simplicity the model should be able to reach above 90%. Also, 40 epochs is not efficient, further investigations and tuning on the learning rate could lead to faster learning, thus I would be able to decrease the number of epochs. However, the confusion matrix generates good results, and the model manages to classify 93.75%, which I will consider as an descent model even though the dataset is very simple. <br>\n",
    "In order to obtain a better accuracy on validation-set, further tuning on kernel size and output channels in the convolutional layers could be done. \n",
    "\n",
    "\n",
    "\n",
    "### Augmentation\n",
    "For this task, I tested for three techniques: RandomHorizontalFlip with both 0.5 and 0.3 as probablity, RandomVerticalFlip and RandomRotation with an angle of 15 degrees. I tried each one seperatly, and a combination of all where the best result was obtained using RandomHorizontalFlip with 0.3 probability separately. The images of the metrics can be shown below: <br><br>\n",
    "\n",
    "<center><strong>Accuracy for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/Augmentation/Accuracy_Train.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/Augmentation/Accuracy_Validation.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "    \n",
    "<br><br>\n",
    "\n",
    "<center><strong>F1-score for train and validation-set</strong></center>\n",
    "<table><tr>\n",
    "    <td><h2 style=\"margin-right:20%;\">Train</h2><img src=\"Images/Augmentation/F1_Train.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></td>\n",
    "    <td><h2 style=\"margin-right:10%;\">Validation</h2><img src=\"Images/Augmentation/F1_Validation.svg\" alt=\"Drawing\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "    \n",
    "<br><br>\n",
    "\n",
    "<center><strong>Confusion matrix on test-data</strong></center>\n",
    "<table><tr>\n",
    "    <td><img src=\"Images/Augmentation/conf.png\" alt=\"Drawing\" style=\"width: 100%;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "***Question 1:*** Did data augmentation improve the model?<br>\n",
    "***Answer 1:*** By looking at the training and validation it did not impact that much. Validation got increased from 85% to almost 88%, which for me is not enough. Though, by looking at the confusion matrix, the test accuracy got improved from 93% to 97%.  <br><br>\n",
    "***Question: 2*** What do you think have the greatest impact on the performance, why? <br>\n",
    "***Answer 2:*** My intention to the performance is that the architecture of the convolutional in the model is the main part that impact the performance, since this is the one part operating on the images. Also, I tried to change the architecture in the feedforward network (hidden nodes) but it did not have that much impact. Small modifications in the convolutional layers made big difference in performance. learning rate and reg term is of big importance. <br><br>\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "This lab gave insights in the operations of an convolution layer, and how the CNN works as a whole. The tuning demonstrated the minior changes that could be done and how the performance of the model changed, e.g. as the conv layers increase the model will underfit and a way too simple model will probably overfit. The final task, showed the impact on the model by applying augmentation, even though this did not cause any big changes as I expected, this could be due to the simple dataset, or that more augmentation techniques should be included in the transformer. Also, from each image, I use both ***Accuracy*** and ***F1-score***, F1-score helps measuring the model performance when working with imbalanced data. From all results, accuracy and f1-score generates same values, therefore accuracy would be a good metric for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
