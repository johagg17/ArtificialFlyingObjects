{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Johannes HÃ¤gglund \\\n",
    "**Date:** 2021-11-15\n",
    "\n",
    "## Introduction\n",
    "For this lab two sub-parts had to be done in order to pass. The first part contained basic tasks regarding neural networks such as activation functions, gradient losses, forward and backward propagation. Based on these investigations and tasks, I also got in task to write my own neural network based on the knowledge I gained in the previous tasks. \\\n",
    "For the second part, I was supposed to create a neural network of n numbers of hidden layers with the use of Pytorch, and then evaluate the model by performing hyperparameter-tuning. \n",
    "\n",
    "## Result\n",
    "In this section I will go through each part including the result I obtained and answers to the questions. \n",
    "### Part1\n",
    "**Question 1:** Can the result be improved? **Answer:** Yes the result can be improved. The current result shows on high accuracy, but this does not tell us that the model generalizes and performs good on unseen data. What I could do is to apply crossvalidation and hyperparameter-tuning in order to find the best and generalized model. <br>\n",
    "\n",
    "**Question 2:** How many epoch are reasonable to run? **Answer:** The number of epochs is hard to estimate, since it firstly depends on the problem, and the model. If the epochs is too small, the model may not be trained as much as it should, and too many epochs may overtrain the model and cause overfitting. Also the number of epochs will effect the computational time. Instead of running the model for a fixed number of epochs, by validation, the training could be stopped as soon as validation loss increase. <br>\n",
    "\n",
    "**Question 3:** How does the performance change if we modify the learning rate? **Answer:** So the learning rate $\\alpha$, is used for the gradient descent step, and in simple terms it defines how \"large\" jumps the algorithm performs in order to find the local minimum. If we select way too small value for the $\\alpha$, then the algorithm will take so small steps that it will result in too long time for finding the local minima.  \n",
    "On the other way, if we chose a way too large value, then we might end up in the scenario where the algorithm never finds the local minima, since the performed steps result in moving away from the local minima. \n",
    "\n",
    "### Part2\n",
    "**Question 1:** How did the Feedforward network perform? **Answer:** <br>\n",
    "\n",
    "**Question 2:** How did the modified Feedforward network perform? **Answer:** <br>\n",
    "\n",
    "**Question 3:** Can we reduce number of nodes with an increased number of layers or vise verse, any changes? **Answer:** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
