{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Johannes HÃ¤gglund \\\n",
    "**Date:** 2021-11-15\n",
    "\n",
    "## Introduction\n",
    "For this lab two sub-parts had to be done in order to pass. The first part contained basic tasks regarding neural networks such as activation functions, gradient losses, forward and backward propagation. Based on these investigations and tasks, I also got in task to write my own neural network based on the knowledge I gained in the previous tasks. \\\n",
    "For the second part, I was supposed to create a neural network of n numbers of hidden layers with the use of Pytorch, and then evaluate the model by performing hyperparameter-tuning. \n",
    "\n",
    "## Result\n",
    "In this section I will go through each part including the result I obtained and answers to the questions. \n",
    "### Part1\n",
    "**Question 1:** Can the result be improved? **Answer:** Yes the result can be improved. The current result shows on high accuracy, but this does not tell us that the model generalizes and performs good on unseen data. What I could do is to apply crossvalidation and hyperparameter-tuning in order to find the best and generalized model. <br>\n",
    "\n",
    "**Question 2:** How many epoch are reasonable to run? **Answer:** The number of epochs is hard to estimate, since it firstly depends on the problem, and the model. If the epochs is too small, the model may not be trained as much as it should, and too many epochs may overtrain the model and cause overfitting. Also the number of epochs will effect the computational time. Instead of running the model for a fixed number of epochs, by validation, the training could be stopped as soon as validation loss increase. <br>\n",
    "\n",
    "**Question 3:** How does the performance change if we modify the learning rate? **Answer:** So the learning rate $\\alpha$, is used for the gradient descent step, and in simple terms it defines how \"large\" jumps the algorithm performs in order to find the local minimum. If we select way too small value for the $\\alpha$, then the algorithm will take so small steps that it will result in too long time for finding the local minima.  \n",
    "On the other way, if we chose a way too large value, then we might end up in the scenario where the algorithm never finds the local minima, since the performed steps result in moving away from the local minima. <br><br>\n",
    "\n",
    "Something to mention is regarding the different tasks that were given in the first part. Most of the tasks were very straightforward, but the gradient_loss task took a bit longer to solve. By reading the linked website regarding [backpropagation](https://dfdazac.github.io/06-neural-networks-numpy.html), the equations for each gradient matrix could be solved.   <br>\n",
    "\n",
    "$$\\nabla w_{o}\\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial z_{o}}a^{T}_{h}, a_{h} = h, \\frac{\\partial \\mathcal{L}}{\\partial z_{o}} = y_{prediction} - y_{true} $$  \n",
    "\n",
    "$$\\nabla b_{o}\\mathcal{L} = mean[\\frac{\\partial \\mathcal{L}}{\\partial z_{o}}] $$ \n",
    "\n",
    "$$\\nabla w_{h}\\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial z_{h}}a^{T}_{h}, a_{h} = X, \\frac{\\partial \\mathcal{L}}{\\partial z_{h}} = (dsigmoid(z_h) * (W_o^{T} \\dot (y_{prediction} - y_{true}))  $$ \n",
    "\n",
    "$$\\nabla b_{h}\\mathcal{L} = mean[\\frac{\\partial \\mathcal{L}}{\\partial z_{h}}] $$ \n",
    "\n",
    "### Part2\n",
    "**Question 1:** How did the Feedforward network perform? **Answer:** For the feedforward network, one hidden layer were created with 2 hidden nodes. Unfortunately this model did not generate any better results, probably due to the low structure it has with 1 hidden layer with 2 hidden nodes.   <br>\n",
    "\n",
    "**Question 2:** How did the modified Feedforward network perform? **Answer:** Firstly, the Feedforward network got extended with different hidden nodes and one hidden layer, which from the results I can tell that, by increasing the hidden nodes to 150 the model performs well, after that the loss increase. <br>\n",
    "Finally hyperparameter-tuning were done by tuning both hidden layers and hidden nodes. The final obtained model was the one with 5 hidden layers, and 100 hidden nodes in each. The final loss for this model was 0.06528, after 5 epochs. <br>\n",
    "Why I obtained the parameters as I did, is probably because lower layers generates a model which is not complex enough for the problem, in this case to create a boundary which is capable of seperating between the data points for each digit in the dataset. <br>\n",
    "\n",
    "**Question 3:** Can we reduce number of nodes with an increased number of layers or vise verse, any changes? <br>\n",
    "**Answer:** For hyperparameter-tuning there will be no different changes, since the pair-wise combination between number of nodes and hidden layers will still be tested.  <br><br> \n",
    "If I were going to think about the concept of hidden layer and hidden nodes in general, the hidden nodes simply defines how many new features the network will learn based on the inputs it gets in each neuron. The hidden layers simply tells how deep and complex problems that should be taught. The deeper network, the more complex features it can learn, e.g. eyes, eye-color, nose etc. Though a way too deep network could result in not just being trained on the distribution within the data, but also the noise which probably will cause the model to get overfitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
